Instalando nuestro ambiente de trabajo

1-Iniciar ubuntu
2-Ingresar el siguiente comando para crear el repositorio:

juandiegorojas@Diego:~$ sudo add-apt-repository ppa:openjdk-r/ppa

3-Refrescar los archivos de los repositorios de nuestro linux

juandiegorojas@Diego:~$ sudo apt-get -y update
juandiegorojas@Diego:~$ sudo apt-get -y upgrade

4-Instalamos java:

juandiegorojas@Diego:~$ sudo apt-get -y install openjdk-8-jre

5-Instalamos python:

juandiegorojas@Diego:~$ sudo apt-get -y install python3

6-Instalamos scala:

juandiegorojas@Diego:~$ sudo apt-get -y install scala

7-Instalamos pip3:

juandiegorojas@Diego:~$ sudo apt-get -y install python3-pip

8-Como spark funciona en la maquina virtual de java usamos el siguiente comando para poder usar python dentro de spark

juandiegorojas@Diego:~$ sudo pip3 install py4j

9-Vamos al siguiente vinculo para instalar apache spark

https://spark.apache.org/downloads.html

10- seleccionamos:
Choose a spark release: 3.3.0
Choose a package type: prebuilt for apache hadoop 2.7
Download spark -> seleccionamos el link y descargamos primer link (archivo.tgz)

11- Una vez descargado descomprimimos archivo y procedemos con la instalacion:
juandiegorojas@Diego:/mnt/c/users/juandiego/downloads$ tar -xvf spark-3.3.0-bin-hadoop2.tgz
juandiegorojas@Diego:/mnt/c/users/juandiego/downloads$ mv spark-3.3.0-bin-hadoop2 spark
juandiegorojas@Diego:/mnt/c/users/juandiego/downloads$ mv spark ~/
juandiegorojas@Diego:/mnt/c/users/juandiego/downloads$ ls ~/
juandiegorojas@Diego:/mnt/c/users/juandiego/downloads$ wget https://repo.anaconda.com/archive/Anaconda3-2020.02-Linux-x86_64.sh
juandiegorojas@Diego:/mnt/c/users/juandiego/downloads$ sh Anaconda3-2020.02-Linux-x86_64.sh -b
juandiegorojas@Diego:/mnt/c/users/juandiego/downloads$ export PATH=/home/juandiegorojas/anaconda3/bin:$PATH
juandiegorojas@Diego:/mnt/c/users/juandiego/downloads$ conda install py4j

12- Crear variables de entorno
juandiegorojas@Diego:~$ nano .bashrc

Introduzco al final del file
## java
export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"
export PATH=$JAVA_HOME/bin:$PATH

##spark
export SPARK_HOME="/home/juandiegorojas/spark"
export PATH=$SPARK_HOME:$PATH
export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
export PYSPARK_PYTHON=python3

13- refrescamos cambios:
juandiegorojas@Diego:/$ source .bashrc
(base) juandiegorojas@Diego:/$ conda deactivate

ejemplo para probar ambiente:
subimos codeExample.py & data.csv a documentos

juandiegorojas@Diego:/mnt/c/users/juandiego/documents$ head -n 10 data.csv  (vemos primeros 10 registros de nuestro archivo data)
juandiegorojas@Diego:/mnt/c/users/juandiego/documents$ nano codeExample.py  (vemos primeros 10 registros de nuestro archivo data)

## Invocamos spark submit (ejecucion de spark mediante linea de comandos)

juandiegorojas@Diego:/mnt/c/users/juandiego/documents$ /home/juandiegorojas/spark/bin/spark-submit codeExample.py data.csv


##### Para ejecutar spark a traves de Jupyter:
##Jupyter
juandiegorojas@Diego:/$ nano ~/.bashrc

agragamos las siguientes variables de entorno al final del file (ctrolx+enter):

export PYSPARK_DRIVER_PYTHON="jupyter"
export PYSPARK_DRIVER_PYTHON_OPTS="notebook"
export PATH=/home/juandiegorojas/anaconda3/bin:$PATH

juandiegorojas@Diego:/$ source ~/.bashrc
(base) juandiegorojas@Diego:/$ conda deactivate

configurar jupyter para linux:
juandiegorojas@Diego:/mnt/c/users/juandiego/documents$ jupyter notebook --generate-config
juandiegorojas@Diego:/mnt/c/users/juandiego/documents$ sudo nano /home/juandiegorojas/.jupyter/jupyter_notebook_config.py

cambiar por la siguiente instruccion:
c.NotebookApp.use_redirect_file = False

ir al archivo ~/.bashrc e introducir el path del browser 
juandiegorojas@Diego:~$ nano ~/.bashrc

export BROWSER='/mnt/c/Program Files (x86)/Google/Chrome/Application/chrome.exe'

juandiegorojas@Diego:~$ source ~/.bashrc

(base) juandiegorojas@Diego:~$ conda deactivate







































